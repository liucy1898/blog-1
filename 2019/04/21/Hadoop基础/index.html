<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/blog/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/blog/css/main.css?v=6.1.0" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png?v=6.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png?v=6.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png?v=6.1.0">


  <link rel="mask-icon" href="/blog/images/logo.svg?v=6.1.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Pisces',
    version: '6.1.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="1. 简介2. Hadoop 安装2.1 JDK 安装配置可以在 Oracle 官网或者直接使用 wget 命令下载 1wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java">
<meta name="keywords" content="Hadoop">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop基础">
<meta property="og:url" content="https://destinywang.github.io/blog/2019/04/21/Hadoop基础/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="1. 简介2. Hadoop 安装2.1 JDK 安装配置可以在 Oracle 官网或者直接使用 wget 命令下载 1wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/56851599-89f87e80-6943-11e9-8b32-2453360ffef9.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/56851719-ef993a80-6944-11e9-93ab-bc181fe1c5ee.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/56866098-2b490880-6a08-11e9-8e16-29f651ccc7a3.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/56866210-4c5e2900-6a09-11e9-8587-949e514572eb.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/56866418-6d277e00-6a0b-11e9-9e87-832ec82a11f6.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/56905677-3cab1700-6ad3-11e9-93bf-b7274eca76ca.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/56906117-2baed580-6ad4-11e9-98b3-f0b1a8b24d94.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/56973756-0f786a80-6ba0-11e9-852e-47688237b2be.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/56974348-426f2e00-6ba1-11e9-9de9-8927285be487.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/56974786-1a33ff00-6ba2-11e9-9575-a52c554de287.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/56975718-f83b7c00-6ba3-11e9-8880-1c968d4d8dbd.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/56976335-50bf4900-6ba5-11e9-9f11-61b04f473601.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/57002668-999dee80-6bf3-11e9-8fcc-bd41e67e0788.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/57017238-d6e49980-6c50-11e9-9aeb-cc3f98d4fd16.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/57017640-91c16700-6c52-11e9-8584-be7726f0d5cc.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/57052874-aafefd80-6cbc-11e9-8d86-0ee8a6022d28.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/57019091-021eb700-6c58-11e9-903b-4aa941551f75.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/57019992-9be76380-6c5a-11e9-8048-d55108068731.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/57020332-8aeb2200-6c5b-11e9-8075-2ffaebfd3096.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/57053421-7c832180-6cc0-11e9-9ac0-cad53ca1ae99.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/58401666-055b6600-8091-11e9-93ca-204b1312832c.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/58405892-0c877180-809b-11e9-9bed-22bb2a046cc3.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/58406172-98999900-809b-11e9-98c9-77d5b598fe54.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/58406539-6b99b600-809c-11e9-8a9e-67819fc8e711.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/58406887-33df3e00-809d-11e9-9ebb-e7e94cdf83f8.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/58407545-9dac1780-809e-11e9-9873-15c03f8832eb.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/58408282-1495e000-80a0-11e9-9cd4-8221cd4cc79a.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/58748384-85773680-84aa-11e9-8233-daebe5f2ccf6.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/58748483-1c90be00-84ac-11e9-8a8f-a1047ab1e791.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/58748603-f5d38700-84ad-11e9-862f-26c9e6a73f13.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/58748576-8d84a580-84ad-11e9-98db-95d5970a38d0.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/60674721-568b2080-9ead-11e9-8c35-654d3fdc8fdc.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/61185171-8ba32a00-a688-11e9-895f-b9b2e2fbaf96.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/61185527-1b4ad780-a68d-11e9-9595-f8a380a57aa7.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/58748384-85773680-84aa-11e9-8233-daebe5f2ccf6.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/17758731/61586498-5fb40700-aba8-11e9-86ff-1309911e1010.png">
<meta property="og:updated_time" content="2019-07-27T01:55:57.663Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hadoop基础">
<meta name="twitter:description" content="1. 简介2. Hadoop 安装2.1 JDK 安装配置可以在 Oracle 官网或者直接使用 wget 命令下载 1wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java">
<meta name="twitter:image" content="https://user-images.githubusercontent.com/17758731/56851599-89f87e80-6943-11e9-8b32-2453360ffef9.png">



  <link rel="alternate" href="/blog/atom.xml" title="Hexo" type="application/atom+xml">




  <link rel="canonical" href="https://destinywang.github.io/blog/2019/04/21/Hadoop基础/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Hadoop基础 | Hexo</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    <a href="https://github.com/destinywang"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"></a>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> 

<div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/blog/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        
          
  <li class="menu-item menu-item-home">
    <a href="/blog/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>首页</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-archives">
    <a href="/blog/archives" rel="section">
      <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>归档</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-categories">
    <a href="/blog/categories" rel="section">
      <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>分类</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-tags">
    <a href="/blog/tags" rel="section">
      <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>标签</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-about">
    <a href="/blog/about" rel="section">
      <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>关于</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-search">
    <a href="/blog/search" rel="section">
      <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>搜索</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-commonweal">
    <a href="/blog/404.html" rel="section">
      <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>公益 404</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-something">
    <a href="/blog/something" rel="section">
      <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>something</a>
</li>

      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://destinywang.github.io/blog/blog/2019/04/21/Hadoop基础/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="destiny">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/blog/images/blog-logo.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Hadoop基础</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-21T10:45:06+08:00">2019-04-21</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/blog/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/blog/categories/大数据/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><h1 id="2-Hadoop-安装"><a href="#2-Hadoop-安装" class="headerlink" title="2. Hadoop 安装"></a>2. Hadoop 安装</h1><h2 id="2-1-JDK-安装配置"><a href="#2-1-JDK-安装配置" class="headerlink" title="2.1 JDK 安装配置"></a>2.1 JDK 安装配置</h2><p>可以在 Oracle 官网或者直接使用 wget 命令下载</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget --no-check-certificate --no-cookies --header <span class="string">"Cookie: oraclelicense=accept-securebackup-cookie"</span> http://download.oracle.com/otn-pub/java/jdk/8u65-b17/jdk-8u65-linux-x64.tar.gz</span><br></pre></td></tr></table></figure>
<p>使用 <code>tar -zxvf</code> 完成解压</p>
<p><img src="https://user-images.githubusercontent.com/17758731/56851599-89f87e80-6943-11e9-8b32-2453360ffef9.png" alt="image"></p>
<p>再将路径 (<code>/usr/soft/jdk1.8.0_65</code>) 配置到 <code>etc/enviroment</code> 路径下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 给 path 新增 JDK 的 bin 路径</span><br><span class="line">PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/soft/jdk1.8.0_65/bin&quot;</span><br><span class="line"></span><br><span class="line"># jdk 路径</span><br><span class="line">JAVA_HOME=/usr/soft/jdk1.8.0_65</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/17758731/56851719-ef993a80-6944-11e9-93ab-bc181fe1c5ee.png" alt="image"></p>
<p>经此验证, 已经成功安装并配置 JDK</p>
<h2 id="2-2-Hadoop-安装配置"><a href="#2-2-Hadoop-安装配置" class="headerlink" title="2.2 Hadoop 安装配置"></a>2.2 Hadoop 安装配置</h2><p>在 Apache 官网下载压缩包并解压:</p>
<p><img src="https://user-images.githubusercontent.com/17758731/56866098-2b490880-6a08-11e9-8e16-29f651ccc7a3.png" alt="image"></p>
<p>在 <code>/etc/enviroment</code> 文件中配置环境变量</p>
<pre><code>PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/soft/jdk1.8.0_65/bin:/usr/soft/hadoop-2.7.7/bin:/usr/soft/hadoop-2.7.7/sbin&quot;
JAVA_HOME=/usr/soft/jdk1.8.0_65
HADOOP_INSTALL=/usr/soft/hadoop-2.7.7
</code></pre><p>在输入 <code>hadoop version</code> 即可看到输出:</p>
<pre><code>destiny@destiny-Parallels-Virtual-Platform:/etc$ hadoop version
Hadoop 2.7.7
Subversion Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac
Compiled by stevel on 2018-07-18T22:47Z
Compiled with protoc 2.5.0
From source with checksum 792e15d20b12c74bd6f19a1fb886490
This command was run using /usr/soft/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar
destiny@destiny-Parallels-Virtual-Platform:/etc$ 
</code></pre><p><img src="https://user-images.githubusercontent.com/17758731/56866210-4c5e2900-6a09-11e9-8587-949e514572eb.png" alt="image"></p>
<h1 id="3-Hadoop-配置"><a href="#3-Hadoop-配置" class="headerlink" title="3. Hadoop 配置"></a>3. Hadoop 配置</h1><p>Hadoop 的配置都是 XML 文件的方式完成, 通用配置都在 <code>core-site.xml</code> 中, HDFS, MapReduce 和 YARN 都有对应的 <code>hdfs-site.xml</code>, <code>mapred-site.xml</code> 以及 <code>yarn-site.xml</code>.</p>
<p>Hadoop 的设计的目的在于处理海量数据, 其主要内容包括数据的存储以及运算, 存储使用 HDFS 实现, 运算使用 MapReduce 编程模型实现.</p>
<p>Hadoop 有三种配置方式:</p>
<ol>
<li>独立模式: 没有守护程序, 所有程序都运行在一个单独的 JVM 之上, 独立模式适合在开发期间运行 MapReduce 程序, 方便调试和测试.</li>
<li>伪分布式: Hadoop 守护程序运行在本地机器上, 会模拟一个小规模的集群.</li>
<li>完全分布式: 运行在集群的不同机器上.</li>
</ol>
<p>当需要运行某个模式的 Hadoop 时, 需要设置适当的配置, 以及启动守护进程(独立模式除外), 不同模式见的配置如下:</p>
<table>
<thead>
<tr>
<th style="text-align:center">配置文件</th>
<th style="text-align:center">属性</th>
<th style="text-align:center">独立模式值</th>
<th style="text-align:center">伪分布式值</th>
<th style="text-align:center">完全分布式值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">core</td>
<td style="text-align:center">fs.defaultFS</td>
<td style="text-align:center">file:///(默认值)</td>
<td style="text-align:center">hdfs://localhost/</td>
<td style="text-align:center">hdfs://namenode</td>
</tr>
<tr>
<td style="text-align:center">HDFS</td>
<td style="text-align:center">dfs.replication</td>
<td style="text-align:center">N/A</td>
<td style="text-align:center">1</td>
<td style="text-align:center">3(默认值)</td>
</tr>
<tr>
<td style="text-align:center">MapReduce</td>
<td style="text-align:center">mapreduce.framework.name</td>
<td style="text-align:center">local(默认值)</td>
<td style="text-align:center">yarn</td>
<td style="text-align:center">yarn</td>
</tr>
<tr>
<td style="text-align:center">yarn</td>
<td style="text-align:center">yarn.resourcemanager.hostname<br>yarn.nodemanager.aux-services</td>
<td style="text-align:center">N/A<br>N/A</td>
<td style="text-align:center">localhost <br> mapreduce_shuffle</td>
<td style="text-align:center">resourcemanager <br> mapreduce_shuffle</td>
</tr>
</tbody>
</table>
<p>此外, Hadoop 的不同配置模式见可以共存, 只需用不同的目录存放配置文件即可, 启动的时候可以通过如下两种方式来指定配置文件:</p>
<ol>
<li>设置 <code>HADOOP_CONF_DIR</code> 环境变量</li>
<li>通过 <code>--config</code> 选项来指定</li>
</ol>
<h2 id="3-1-独立模式"><a href="#3-1-独立模式" class="headerlink" title="3.1 独立模式"></a>3.1 独立模式</h2><p>在独立模式下不需要进行额外的配置, 所有默认的属性都是针对独立模式的, 也没有守护程序运行, 独立模式下使用的文件系统是 <code>Local File System</code> 和 <code>Local MR job runner</code>.</p>
<p><img src="https://user-images.githubusercontent.com/17758731/56866418-6d277e00-6a0b-11e9-9e87-832ec82a11f6.png" alt="image"></p>
<p>可以看到, 独立模式下使用 <code>hadoop fs -ls</code> 显示的就是本机的根路径文件</p>
<h2 id="3-2-伪分布式"><a href="#3-2-伪分布式" class="headerlink" title="3.2 伪分布式"></a>3.2 伪分布式</h2><h3 id="3-2-1-配置文件"><a href="#3-2-1-配置文件" class="headerlink" title="3.2.1 配置文件"></a>3.2.1 配置文件</h3><p>在伪分布式环境下, 需要配置如下文件:</p>
<ol>
<li><p>core-site.xml(核心站点)</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>mapred-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- yarn 是一个 MapReduce 框架, 2.0 版本以上开始引入 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>初始状态下, 这些配置文件中都是空值, 需要根据使用者的自身情况去配置以完成不同环境的搭建.</p>
<p><img src="https://user-images.githubusercontent.com/17758731/56905677-3cab1700-6ad3-11e9-93bf-b7274eca76ca.png" alt="image"></p>
<p>我们将 <code>$HADOOP_INSTALL/etc/hadoop</code> 文件夹拷贝一份, 用作伪分布式的配置</p>
<p><img src="https://user-images.githubusercontent.com/17758731/56906117-2baed580-6ad4-11e9-98b3-f0b1a8b24d94.png" alt="image"></p>
<p>然后依次将上文提到的四个配置文件修改成指定的配置方式</p>
<h3 id="3-2-2-配置-SSH"><a href="#3-2-2-配置-SSH" class="headerlink" title="3.2.2 配置 SSH"></a>3.2.2 配置 SSH</h3><p>在伪分布式下, 必须要启动守护进程, 启动守护进程就需要使用提供的启动脚本, Hadoop 并不严格区分伪分布式和完全分布式, 只是在目标主机上启动守护进程, 通过 SSH 命令让主机之间相互通信, 而且要启动守护进程. </p>
<p>伪分布式只是完全分布式的一个特例, 是一个在单个主机上运行 Hadoop 完全分布式的场景, 因此我们需要确保能够通过 SSH 命令登录本机, 而不需要通过输入密码.</p>
<p>在 ubuntu 上可以通过 <code>sudo apt-get install ssh</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install ssh                  <span class="comment"># 安装 ssh</span></span><br><span class="line">$ ssh-keygen -t rsa -P <span class="string">''</span> -f ~/.ssh/id_rsa  <span class="comment"># 生成公钥和私钥, -P '' 代表指定密码为空</span></span><br><span class="line">$ cat id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys  <span class="comment"># 将公钥配置给 authorized_keys, 用来实现免密登陆</span></span><br><span class="line">$ ssh localhost                             <span class="comment"># 第一次yes</span></span><br><span class="line">$ yes</span><br><span class="line">$ ssh localhost                             <span class="comment"># 第二次不需要口令</span></span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/17758731/56973756-0f786a80-6ba0-11e9-852e-47688237b2be.png" alt="image"></p>
<h3 id="3-2-3-使用-HDFS"><a href="#3-2-3-使用-HDFS" class="headerlink" title="3.2.3 使用 HDFS"></a>3.2.3 使用 HDFS</h3><p>首先需要对文件系统进行格式化</p>
<pre><code>hdfs namenode -format
</code></pre><p>然后就可以启动守护进程</p>
<h4 id="3-2-3-1-启动"><a href="#3-2-3-1-启动" class="headerlink" title="3.2.3.1 启动"></a>3.2.3.1 启动</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line">start-dfs.sh --config <span class="variable">$HADOOP_INSTALL</span>/etc/hadoop_pseudo</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/17758731/56974348-426f2e00-6ba1-11e9-9de9-8927285be487.png" alt="image"></p>
<h4 id="3-2-3-2-启动-yarn"><a href="#3-2-3-2-启动-yarn" class="headerlink" title="3.2.3.2 启动 yarn"></a>3.2.3.2 启动 yarn</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line">start-yarn.sh --config <span class="variable">$HADOOP_INSTALL</span>/etc/hadoop_pseudo</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/17758731/56974786-1a33ff00-6ba2-11e9-9575-a52c554de287.png" alt="image"></p>
<p>此时可以通过 jps 查看:</p>
<p><img src="https://user-images.githubusercontent.com/17758731/56975718-f83b7c00-6ba3-11e9-8880-1c968d4d8dbd.png" alt="image"></p>
<ul>
<li>其中 ResourceManager 和 NodeManager 由 Yarn 提供</li>
<li>NameNode, DataNode, SecondaryNameNode 由 HDFS 提供</li>
</ul>
<p>可以使用如下命令停止 Hadoop<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stop-yarn.sh</span><br><span class="line">stop-dfs.sh</span><br></pre></td></tr></table></figure></p>
<p>通过设置环境变量, 可以不再需要借助 –config</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/usr/soft/hadoop-2.7.7/etc/hadoop_pseudo</span><br></pre></td></tr></table></figure>
<p>此时再使用 <code>hadoop fs -ls /</code> 就已经没有结果显示</p>
<p>我们可以像使用 Linux 系统类似的命令去操作 HDFS, 下图展示一个创建文件夹的操作</p>
<p><img src="https://user-images.githubusercontent.com/17758731/56976335-50bf4900-6ba5-11e9-9f11-61b04f473601.png" alt="image"></p>
<h2 id="3-3-完全分布式"><a href="#3-3-完全分布式" class="headerlink" title="3.3 完全分布式"></a>3.3 完全分布式</h2><h3 id="3-3-1-准备工作"><a href="#3-3-1-准备工作" class="headerlink" title="3.3.1 准备工作"></a>3.3.1 准备工作</h3><ol>
<li><p>在 /etc/passwd 修改登录提示消息</p>
</li>
<li><p>在 /etc/hostname 中修改主机名</p>
</li>
</ol>
<p>可以通过软连接的方式指定 Hadoop 配置文件</p>
<p><img src="https://user-images.githubusercontent.com/17758731/57002668-999dee80-6bf3-11e9-8fcc-bd41e67e0788.png" alt="image"></p>
<p>将现在的 ubuntu 虚拟机克隆出三份, 具体配置如下:</p>
<p><img src="https://user-images.githubusercontent.com/17758731/57017238-d6e49980-6c50-11e9-9aeb-cc3f98d4fd16.png" alt="image"></p>
<ol>
<li>左边标出本人当前的四台节点 ip</li>
<li><p>将 ip 和编号分别写在当前节点(s1) 的 <code>/etc/hosts</code> 文件中:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">10.211.55.33 s0</span><br><span class="line">10.211.55.34 s1</span><br><span class="line">10.211.55.35 s2</span><br><span class="line">10.211.55.36 s3</span><br></pre></td></tr></table></figure>
</li>
<li><p>此时测试, s0, s1, s2, s3 均可以被 ping 通</p>
</li>
<li>现在需要将 hosts 分别同步(覆盖)给到 s1, s2, s3 节点</li>
</ol>
<p><img src="https://user-images.githubusercontent.com/17758731/57017640-91c16700-6c52-11e9-8584-be7726f0d5cc.png" alt="image"></p>
<p>Hadoop 集群架构分析</p>
<p><img src="https://user-images.githubusercontent.com/17758731/57052874-aafefd80-6cbc-11e9-8d86-0ee8a6022d28.png" alt="image"></p>
<h3 id="3-3-2-集群模式配置"><a href="#3-3-2-集群模式配置" class="headerlink" title="3.3.2 集群模式配置"></a>3.3.2 集群模式配置</h3><p>预期部署的网络拓扑图: </p>
<p><img src="https://user-images.githubusercontent.com/17758731/57019091-021eb700-6c58-11e9-903b-4aa941551f75.png" alt="image"></p>
<table>
<thead>
<tr>
<th>节点名</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>s0</td>
<td>名称节点</td>
</tr>
<tr>
<td>s1</td>
<td>数据节点</td>
</tr>
<tr>
<td>s2</td>
<td>数据节点</td>
</tr>
<tr>
<td>s3</td>
<td>辅助名称节点</td>
</tr>
</tbody>
</table>
<blockquote>
<p>下面我们从 s0 开始进行配置, 在完成 s0 的配置后, Hadoop 运行时需要所有节点的配置相同, 因此类似于 hosts 文件, 我们需要将配置好的文件覆盖到其他节点.</p>
</blockquote>
<h4 id="3-3-2-1-core-site-xml"><a href="#3-3-2-1-core-site-xml" class="headerlink" title="3.3.2.1 core-site.xml"></a>3.3.2.1 core-site.xml</h4><p>core-site.xml 用来配置 NameNode 所运行的节点 ip</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://s0/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="3-3-2-2-hdfs-site-xml"><a href="#3-3-2-2-hdfs-site-xml" class="headerlink" title="3.3.2.2 hdfs-site.xml"></a>3.3.2.2 hdfs-site.xml</h4><p>hdfs-site.xml 用来配置 HDFS 的副本数量</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="3-3-2-4-yarn-site-xml"><a href="#3-3-2-4-yarn-site-xml" class="headerlink" title="3.3.2.4 yarn-site.xml"></a>3.3.2.4 yarn-site.xml</h4><p>yarn-site.xml 用来配置 yarn 的资源管理节点</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>s0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="3-3-2-5-slaves"><a href="#3-3-2-5-slaves" class="headerlink" title="3.3.2.5 slaves"></a>3.3.2.5 slaves</h4><p>通过该文件配置从节点(DataNode) 的 ip</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s1</span><br><span class="line">s2</span><br></pre></td></tr></table></figure>
<p>最后再将整个 <code>hadoop_cluster</code> 文件夹覆盖到其他节点</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo scp -r hadoop_cluster root@s1:/usr/soft/hadoop-2.7.7/etc/</span><br><span class="line">sudo scp -r hadoop_cluster root@s2:/usr/soft/hadoop-2.7.7/etc/</span><br><span class="line">sudo scp -r hadoop_cluster root@s3:/usr/soft/hadoop-2.7.7/etc/</span><br></pre></td></tr></table></figure>
<p>使用 <code>start-all.sh</code> 完成 hdfs 和 yarn 的启动:</p>
<p><img src="https://user-images.githubusercontent.com/17758731/57019992-9be76380-6c5a-11e9-8048-d55108068731.png" alt="image"></p>
<p>我们可以对启动日志做一个简单的解读:</p>
<ul>
<li>在 s0 节点上启动名称节点</li>
<li>分别在 s1 和 s2 启动数据节点</li>
<li>启动辅助名称节点</li>
<li>启动 yarn 守护进程</li>
<li>分别在 s1 和 s2 启动节点管理器</li>
</ul>
<p>可以看一下此时集群上所有节点的 Java 进程状态</p>
<p><img src="https://user-images.githubusercontent.com/17758731/57020332-8aeb2200-6c5b-11e9-8075-2ffaebfd3096.png" alt="image"></p>
<p>此时我们已经完成了 Hadoop 完全分布式配置.</p>
<h1 id="4-分布式文件系统HDFS"><a href="#4-分布式文件系统HDFS" class="headerlink" title="4. 分布式文件系统HDFS"></a>4. 分布式文件系统HDFS</h1><h2 id="4-1-概念"><a href="#4-1-概念" class="headerlink" title="4.1 概念"></a>4.1 概念</h2><ul>
<li>Hadoop 实现的分布式文件系统(Hadoop Distributed File System), 简称 HDFS</li>
<li>源自于 Google 的 GFS 论文</li>
<li>发表与 2003 年, HDFS 是 GFS 的克隆版</li>
</ul>
<h2 id="4-2-设计目标"><a href="#4-2-设计目标" class="headerlink" title="4.2 设计目标"></a>4.2 设计目标</h2><ul>
<li>非常巨大的分布式文件系统</li>
<li>运行在普通的廉价硬件之上</li>
<li>易扩展, 为用户提供性能较高的文件存储服务</li>
</ul>
<h2 id="4-3-HDFS-架构"><a href="#4-3-HDFS-架构" class="headerlink" title="4.3 HDFS 架构"></a>4.3 HDFS 架构</h2><p><img src="https://user-images.githubusercontent.com/17758731/57053421-7c832180-6cc0-11e9-9ac0-cad53ca1ae99.png" alt="image"></p>
<p>一个 Master(NameNode/NN), 以及多个 Slave(DataNode/DN), DataNode 用于管理数据的读写, 一个文件会被拆分成多个 block, 默认为 128M, 被存储在一系列的 DataNode(不是一个 DataNode).</p>
<p>NameNode 职责:</p>
<ol>
<li>负责客户端请求的响应</li>
<li>负责元数据(文件名称, 副本系数, block 存放的 DN)的管理</li>
</ol>
<p>DataNode 职责:</p>
<ol>
<li>存储用户文件对应的数据块</li>
<li>要定期向 NameNode 发送心跳信息, 汇报本身及其所有的 block 信息, 健康状况</li>
</ol>
<h2 id="4-4-HDFS-副本机制"><a href="#4-4-HDFS-副本机制" class="headerlink" title="4.4 HDFS 副本机制"></a>4.4 HDFS 副本机制</h2><p><img src="https://user-images.githubusercontent.com/17758731/58401666-055b6600-8091-11e9-93ca-204b1312832c.png" alt="image"></p>
<p>心跳包中包含的信息:</p>
<ul>
<li>文件名称, 副本系数, block id</li>
<li>e.g. 文件名 part-0, 副本系数 2, block id 为 {1, 3}</li>
<li>e.g. 文件名 part-1, 副本系数 3, block id 为 {2, 4, 5}</li>
</ul>
<blockquote>
<p>HDFS 副本存放策略: 第一个副本存放在客户端所在的节点, 另外两个副本优先存放在不同的机架上, 假设集群只有一个机架, 所有副本都会存放在同一个机架, 如果集群存在多个机架, 就随机挑选一个.</p>
</blockquote>
<h2 id="4-5-Hadoop-Shell"><a href="#4-5-Hadoop-Shell" class="headerlink" title="4.5 Hadoop Shell"></a>4.5 Hadoop Shell</h2><p>基本的命令格式:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs/hadoop fs [generic options]</span><br><span class="line">比如 ls 命令可以写成: hdfs dfs -ls / 或 hadoop fs -ls /</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:center">命令</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">ls</td>
<td style="text-align:center">展示文件/文件夹列表</td>
</tr>
<tr>
<td style="text-align:center">mkdir</td>
<td style="text-align:center">创建文件夹</td>
</tr>
<tr>
<td style="text-align:center">put</td>
<td style="text-align:center">上传文件</td>
</tr>
<tr>
<td style="text-align:center">get</td>
<td style="text-align:center">获取文件</td>
</tr>
<tr>
<td style="text-align:center">rm</td>
<td style="text-align:center">删除文件/文件夹</td>
</tr>
</tbody>
</table>
<h2 id="4-6-通过代码操作-API"><a href="#4-6-通过代码操作-API" class="headerlink" title="4.6 通过代码操作 API"></a>4.6 通过代码操作 API</h2><h3 id="4-6-1-测试文件的通用代码"><a href="#4-6-1-测试文件的通用代码" class="headerlink" title="4.6.1 测试文件的通用代码"></a>4.6.1 测试文件的通用代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HDFSApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String HDFS_PATH = <span class="string">"hdfs://10.211.55.33:8020"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 对于文件系统, 所有操作的统一入库</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    FileSystem fileSystem = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    Configuration configuration = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUp</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"HDFSApp.setUp"</span>);</span><br><span class="line">        configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        fileSystem = FileSystem.get(<span class="keyword">new</span> URI(HDFS_PATH), configuration);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">tearDown</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        configuration = <span class="keyword">null</span>;</span><br><span class="line">        fileSystem = <span class="keyword">null</span>;</span><br><span class="line">        System.out.println(<span class="string">"HDFSApp.tearDown"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-6-2-创建路径"><a href="#4-6-2-创建路径" class="headerlink" title="4.6.2 创建路径"></a>4.6.2 创建路径</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建 HDFS 目录</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mkdir</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/hdfsapi/test"</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行后的结果:</p>
<p><img src="https://user-images.githubusercontent.com/17758731/58405892-0c877180-809b-11e9-9bed-22bb2a046cc3.png" alt="image"></p>
<h3 id="4-6-3-创建文件"><a href="#4-6-3-创建文件" class="headerlink" title="4.6.3 创建文件"></a>4.6.3 创建文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建文件</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    FSDataOutputStream fsDataOutputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/hdfsapi/test/a.txt"</span>));</span><br><span class="line">    fsDataOutputStream.write(<span class="string">"hello hadoop"</span>.getBytes());</span><br><span class="line">    fsDataOutputStream.flush();</span><br><span class="line">    fsDataOutputStream.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行后的结果:</p>
<p><img src="https://user-images.githubusercontent.com/17758731/58406172-98999900-809b-11e9-98c9-77d5b598fe54.png" alt="image"></p>
<h3 id="4-6-4-查看文件内容"><a href="#4-6-4-查看文件内容" class="headerlink" title="4.6.4 查看文件内容"></a>4.6.4 查看文件内容</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 查看 HDFS 文件的内容</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cat</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    FSDataInputStream fsDataInputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/hdfsapi/test/a.txt"</span>));</span><br><span class="line">    IOUtils.copyBytes(fsDataInputStream, System.out, <span class="number">1024</span>);</span><br><span class="line">    fsDataInputStream.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-6-5-重命名"><a href="#4-6-5-重命名" class="headerlink" title="4.6.5 重命名"></a>4.6.5 重命名</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 重命名</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">rename</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> rename = fileSystem.rename(<span class="keyword">new</span> Path(<span class="string">"/hdfsapi/test/a.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"/hdfsapi/test/b.txt"</span>));</span><br><span class="line">    System.out.println(<span class="string">"rename = "</span> + rename);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行结果:</p>
<p><img src="https://user-images.githubusercontent.com/17758731/58406539-6b99b600-809c-11e9-8a9e-67819fc8e711.png" alt="image"></p>
<h3 id="4-6-6-将本地文件-copy-到-HDFS"><a href="#4-6-6-将本地文件-copy-到-HDFS" class="headerlink" title="4.6.6 将本地文件 copy 到 HDFS"></a>4.6.6 将本地文件 copy 到 HDFS</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将文件从本地 copy 到 HDFS</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Path localPath = <span class="keyword">new</span> Path(<span class="string">"/Users/destiny/dev/apache-tomcat-8.5.29-src.tar.gz"</span>);</span><br><span class="line">    Path hdfsPath = <span class="keyword">new</span> Path(<span class="string">"/hdfsapi/test/"</span>);</span><br><span class="line">    fileSystem.copyFromLocalFile(localPath, hdfsPath);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行结果:</p>
<p><img src="https://user-images.githubusercontent.com/17758731/58406887-33df3e00-809d-11e9-9ebb-e7e94cdf83f8.png" alt="image"></p>
<h3 id="4-6-7-带进度条的上传"><a href="#4-6-7-带进度条的上传" class="headerlink" title="4.6.7 带进度条的上传"></a>4.6.7 带进度条的上传</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将文件从本地 copy 到 HDFS</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copyFromLocalFileWithProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    InputStream inputStream = <span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">"/Users/destiny/dev/hadoop-2.7.7.tar.gz"</span>)));</span><br><span class="line">    FSDataOutputStream fsDataOutputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/hdfsapi/test/hadoop-2.7.7.tar.gz"</span>), <span class="keyword">new</span> Progressable() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">progress</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 带进度条提醒信息</span></span><br><span class="line">            System.out.print(<span class="string">"#"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    IOUtils.copyBytes(inputStream, fsDataOutputStream, <span class="number">4096</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行结果:</p>
<p><img src="https://user-images.githubusercontent.com/17758731/58407545-9dac1780-809e-11e9-9873-15c03f8832eb.png" alt="image"></p>
<h3 id="4-6-8-下载-HDFS-文件"><a href="#4-6-8-下载-HDFS-文件" class="headerlink" title="4.6.8 下载 HDFS 文件"></a>4.6.8 下载 HDFS 文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 下载 HDFS 文件到本地</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Path localPath = <span class="keyword">new</span> Path(<span class="string">"/Users/destiny/dev/"</span>);</span><br><span class="line">    Path hdfsPath = <span class="keyword">new</span> Path(<span class="string">"/hdfsapi/test/b.txt"</span>);</span><br><span class="line">    fileSystem.copyToLocalFile(hdfsPath, localPath);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-6-9-查看指定路径下的所有文件"><a href="#4-6-9-查看指定路径下的所有文件" class="headerlink" title="4.6.9 查看指定路径下的所有文件"></a>4.6.9 查看指定路径下的所有文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 查看指定路径的所有文件</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    FileStatus[] fileStatuses = fileSystem.listStatus(<span class="keyword">new</span> Path(<span class="string">"/hdfsapi/test/"</span>));</span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">        <span class="keyword">boolean</span> directory = fileStatus.isDirectory();</span><br><span class="line">        System.out.println(<span class="string">"directory = "</span> + directory);</span><br><span class="line">        <span class="keyword">short</span> replication = fileStatus.getReplication();</span><br><span class="line">        System.out.println(<span class="string">"replication = "</span> + replication);</span><br><span class="line">        <span class="keyword">long</span> len = fileStatus.getLen();</span><br><span class="line">        System.out.println(<span class="string">"len = "</span> + len);</span><br><span class="line">        String path = fileStatus.getPath().toString();</span><br><span class="line">        System.out.println(<span class="string">"path = "</span> + path);</span><br><span class="line">        System.out.println(<span class="string">"==============================="</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行结果:</p>
<p><img src="https://user-images.githubusercontent.com/17758731/58408282-1495e000-80a0-11e9-9cd4-8221cd4cc79a.png" alt="image"></p>
<p>这里有一个小问题:</p>
<blockquote>
<p>在前面的分布式配置中, <code>hdfs-site.xml</code> 中设置的副本系数为 2, 但这里查询到的结果却为 3<br>如果是通过 HDFS shell 的方式 put 上去, 那么会采用设置的副本系数 2<br>而如果是通过 java API 上传, 那么由于本地没有设置副本系数, 因此采用的是 Hadoop 自带的副本系数</p>
</blockquote>
<h3 id="4-6-10-删除文件"><a href="#4-6-10-删除文件" class="headerlink" title="4.6.10 删除文件"></a>4.6.10 删除文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 删除文件(默认递归)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    fileSystem.delete(<span class="keyword">new</span> Path(<span class="string">"/hdfsapi/test/"</span>), <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="4-7-HDFS-文件写入流程"><a href="#4-7-HDFS-文件写入流程" class="headerlink" title="4.7 HDFS 文件写入流程"></a>4.7 HDFS 文件写入流程</h2><h2 id="4-8-HDFS-文件读取刘晨"><a href="#4-8-HDFS-文件读取刘晨" class="headerlink" title="4.8 HDFS 文件读取刘晨"></a>4.8 HDFS 文件读取刘晨</h2><h1 id="5-资源调度框架-YARN"><a href="#5-资源调度框架-YARN" class="headerlink" title="5. 资源调度框架 YARN"></a>5. 资源调度框架 YARN</h1><h2 id="5-1-背景"><a href="#5-1-背景" class="headerlink" title="5.1 背景"></a>5.1 背景</h2><h3 id="5-1-1-MapReduce-1-X-存在的问题"><a href="#5-1-1-MapReduce-1-X-存在的问题" class="headerlink" title="5.1.1 MapReduce 1.X 存在的问题"></a>5.1.1 MapReduce 1.X 存在的问题</h3><blockquote>
<ul>
<li>集群由一个 JobTracker 与多个 TaskTracker 构成, 客户端提交任务的时候, 直接将作业提交给 JobTracker, 由 JobTracker 负责资源的管理与作业的调度;  </li>
<li>TaskTracker 定期通过心跳机制与 JobTracker 进行通信, 汇报健康状况, 资源使用情况以及任务的执行进度, 并且接收来自 JobTracker 的命令来进行任务的启动和结束</li>
</ul>
</blockquote>
<p><img src="https://user-images.githubusercontent.com/17758731/58748384-85773680-84aa-11e9-8233-daebe5f2ccf6.png" alt="image"></p>
<p>存在问题:</p>
<ol>
<li>JobTracker 存在单点故障</li>
<li>JobTracker 负载较大, 需要接收 TaskTracker 的心跳信息, 制约 Hadoop 集群的扩展</li>
<li>JobTracker 承载职责较多,  包括资源管理, 资源调度, 任务分配</li>
<li>仅仅支持 MapReduce 作业, Spark 以及 Storm 作业无法支持</li>
</ol>
<h3 id="5-1-2-资源利用-amp-运维成本"><a href="#5-1-2-资源利用-amp-运维成本" class="headerlink" title="5.1.2 资源利用&amp;运维成本"></a>5.1.2 资源利用&amp;运维成本</h3><p><img src="https://user-images.githubusercontent.com/17758731/58748483-1c90be00-84ac-11e9-8a8f-a1047ab1e791.png" alt="image"></p>
<p>由于 Hadoop 集群不支持其他形式的作业, 因此生产环境需要部署多套集群, 而不同集群在造成更多资源占用的同时, 往往存在运行时间不同, 如果能够将多个集群整合在一起, 就可以节约计算资源.</p>
<p>如果存在一种 <code>共享集群</code>, 能够处理不同类型的作业, 并且能够自行实现资源的合理分配, 就可以解决不同作业任务需要多套集群环境的问题.</p>
<p><img src="https://user-images.githubusercontent.com/17758731/58748603-f5d38700-84ad-11e9-862f-26c9e6a73f13.png" alt="image"></p>
<p>在 Hadoop2.0 的架构中, Hadoop 之上运行 YARN, YARN 负责集群的资源管理, 而 YARN 可以接收来自 MapReduce, HBase, Storm, Spark 等多种应用的输入. YARN 做了统一的抽象, 类似操作系统级别的通用资源调度框架, 可以让更多的计算框架运行在同一个集群中, 不同的计算框架可以共享同一个 HDFS 上的数据, 享受整体的资源调度.</p>
<h2 id="5-2-架构"><a href="#5-2-架构" class="headerlink" title="5.2 架构"></a>5.2 架构</h2><ul>
<li>Yarn(Yet Another Resource Negotiator, 另一个资源协调者的简称)</li>
<li>是一个通用的资源管理系统</li>
<li>为上层应用提供统一的资源管理和调度</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/17758731/58748576-8d84a580-84ad-11e9-98db-95d5970a38d0.png" alt="image"></p>
<p>Yarn 架构的核心组件:</p>
<table>
<thead>
<tr>
<th style="text-align:center">角色</th>
<th>描述</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">ResourceManager</td>
<td>整个集群同一时间提供服务的 Resource Manager 只有一个, 负责集群资源的统一管理和调度</td>
<td>1. 提交作业<br>2. 杀死作业<br>3. 监控 NodeManager, 一旦某个 NodeManager 挂了, 该 NameNode 上运行的任务要告诉 Application Master</td>
</tr>
<tr>
<td style="text-align:center">NodeManager</td>
<td>整个集群中有多个 NodeManager, 负责当前节点资源管理和使用</td>
<td>1. 定时向 ResourceManager 汇报当前节点的资源使用情况<br>2. 接受并处理 ResourceManager 的各种命令<br>3. 处理来自 Application Master 的命令<br>4. 单个节点的资源管理</td>
</tr>
<tr>
<td style="text-align:center">Application Master</td>
<td>每个应用程序对应一个 Application Master, 负责应用程序的管理</td>
<td>1. 为应用程序向 ResourceManager 申请资源(core, mem)<br>2. 分配给内部的 Task 处理<br> 3. 需要与 NodeManager 通信, 启动/停止 task</td>
</tr>
<tr>
<td style="text-align:center">Container</td>
<td>封装了 CPU, MEM 等资源的容器, 是一个任务运行环境的抽象</td>
</tr>
<tr>
<td style="text-align:center">Client</td>
<td>用于封装用户的操作</td>
<td>1. 提交作业<br> 2. 查询作业运行进度 <br> 3. 杀死作业<br></td>
</tr>
</tbody>
</table>
<h2 id="5-3-执行流程"><a href="#5-3-执行流程" class="headerlink" title="5.3 执行流程"></a>5.3 执行流程</h2><p><img src="https://user-images.githubusercontent.com/17758731/60674721-568b2080-9ead-11e9-8c35-654d3fdc8fdc.png" alt="image"></p>
<ol>
<li>用户向 YARN 提交作业</li>
<li>ResourceManager 为作业分配第一个 Container, 与对应的 NodeManager 通信, 要求在其上启动 Container, 用来启动应用程序</li>
<li>NodeManager 按照要求, 启动 Application Master</li>
<li>Application Master 启动后, 会首先在 ResourceManager 进行注册, 此时就可以通过 ResourceManager 查询作业的运行情况. 然后 ApplicationMaster 会将所需要的资源到 ResourceManager 上去申请</li>
<li>ApplicationMaster 申请到资源之后在对应的 NodeManager 上开始启动任务, 所有的任务都是以 Container 的方式运行的</li>
<li>NodeManager 启动对应的 Container 去执行任务.</li>
</ol>
<h2 id="5-5-提交作业到-YARN-上执行"><a href="#5-5-提交作业到-YARN-上执行" class="headerlink" title="5.5 提交作业到 YARN 上执行"></a>5.5 提交作业到 YARN 上执行</h2><h3 id="5-5-1-提交-Hadoop-包的-Example"><a href="#5-5-1-提交-Hadoop-包的-Example" class="headerlink" title="5.5.1 提交 Hadoop 包的 Example"></a>5.5.1 提交 Hadoop 包的 Example</h3><p>在 Hadoop 安装路径下的 <code>share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar</code> 文件</p>
<p>使用如下命令将 MapReduce 作业提交到 YARN 上去运行: </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-2.7.7.jar pi 2 3</span><br></pre></td></tr></table></figure>
<h1 id="6-启动脚本分析"><a href="#6-启动脚本分析" class="headerlink" title="6. 启动脚本分析"></a>6. 启动脚本分析</h1><p>Hadoop 启动脚本位于 <code>${HADOOP_HOME}/bin</code>, <code>${HADOOP_HOME}/sbin</code> 和 <code>${HADOOP_HOME}/libexec</code> 路径下, 其中包含 *nux 的 Shell 脚本和 win 的批处理文件.</p>
<h2 id="6-1-start-all-sh-启动分析"><a href="#6-1-start-all-sh-启动分析" class="headerlink" title="6.1 start-all.sh 启动分析"></a>6.1 start-all.sh 启动分析</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="comment"># contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="comment"># this work for additional information regarding copyright ownership.</span></span><br><span class="line"><span class="comment"># The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="comment"># (the "License"); you may not use this file except in compliance with</span></span><br><span class="line"><span class="comment"># the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Start all hadoop daemons.  Run this on master node.</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh"</span></span><br><span class="line"></span><br><span class="line">bin=`dirname <span class="string">"<span class="variable">$&#123;BASH_SOURCE-$0&#125;</span>"</span>`</span><br><span class="line">bin=`<span class="built_in">cd</span> <span class="string">"<span class="variable">$bin</span>"</span>; <span class="built_in">pwd</span>`</span><br><span class="line"></span><br><span class="line">DEFAULT_LIBEXEC_DIR=<span class="string">"<span class="variable">$bin</span>"</span>/../libexec</span><br><span class="line">HADOOP_LIBEXEC_DIR=<span class="variable">$&#123;HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR&#125;</span></span><br><span class="line">. <span class="variable">$HADOOP_LIBEXEC_DIR</span>/hadoop-config.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># start hdfs daemons if hdfs is present</span></span><br><span class="line"><span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$&#123;HADOOP_HDFS_HOME&#125;</span>"</span>/sbin/start-dfs.sh ]; <span class="keyword">then</span></span><br><span class="line">  <span class="string">"<span class="variable">$&#123;HADOOP_HDFS_HOME&#125;</span>"</span>/sbin/start-dfs.sh --config <span class="variable">$HADOOP_CONF_DIR</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># start yarn daemons if yarn is present</span></span><br><span class="line"><span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$&#123;HADOOP_YARN_HOME&#125;</span>"</span>/sbin/start-yarn.sh ]; <span class="keyword">then</span></span><br><span class="line">  <span class="string">"<span class="variable">$&#123;HADOOP_YARN_HOME&#125;</span>"</span>/sbin/start-yarn.sh --config <span class="variable">$HADOOP_CONF_DIR</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<ol>
<li>首先会通过 echo 输出一句话, 大意是该脚本已被废弃, 推荐使用 <code>start-dfs.sh</code> 和 <code>start-yarn.sh</code></li>
<li><code>bin=`dirname &quot;${BASH_SOURCE-$0}&quot;`</code> 提取 start-all.sh 所在的绝对路径</li>
<li><code>bin=`cd &quot;$bin&quot;; pwd`</code> 切换到 start-all.sh 所在的绝对路径</li>
<li><code>DEFAULT_LIBEXEC_DIR=&quot;$bin&quot;/../libexec</code> 获取 <code>${HADOOP_HOME}/libexec/hadoop-config.sh</code> 路径</li>
<li><code>HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}</code> 为 <code>HADOOP_LIBEXEC_DIR</code> 变量三元赋值: 如果 <code>HADOOP_LIBEXEC_DIR</code> 为空或者环境变量没有配置, 复制默认的绝对路径</li>
<li><code>. $HADOOP_LIBEXEC_DIR/hadoop-config.sh</code> 执行 <code>${HADOOP_HOME}/libexec/hadoop-config.sh</code> 脚本, 为后面执行启动各节点和启动 yarn 做预处理</li>
<li><code>&quot;${HADOOP_HDFS_HOME}&quot;/sbin/start-dfs.sh --config $HADOOP_CONF_DIR</code> 如果 <code>${HADOOP_HDFS_HOME}&quot;/sbin/start-dfs.sh</code> 是文件, 就通过 –config 参数启动 <code>start-dfs.sh</code> 脚本</li>
<li><code>&quot;${HADOOP_YARN_HOME}&quot;/sbin/start-yarn.sh --config $HADOOP_CONF_DIR</code>: 如果 <code>${HADOOP_YARN_HOME}&quot;/sbin/start-yarn.sh</code> 是文件, 就通过 –config 参数启动 <code>start-yarn.sh</code> 脚本</li>
</ol>
<h2 id="6-2-hadoop-config-sh"><a href="#6-2-hadoop-config-sh" class="headerlink" title="6.2 hadoop-config.sh"></a>6.2 hadoop-config.sh</h2><p>该脚本位于</p>
<h1 id="7-MapReduce"><a href="#7-MapReduce" class="headerlink" title="7. MapReduce"></a>7. MapReduce</h1><p>MapReduce 是一种并行计算编程模型, 源自于 Google 的 MapReduce 论文, 包含 Map 过程和 Reduce 过程, Map 过程对应创建 Mapper 实现类, Reduce 过程对应创建 Reducer 实现类.</p>
<h2 id="7-2-Map-和-Reduce-阶段"><a href="#7-2-Map-和-Reduce-阶段" class="headerlink" title="7.2 Map 和 Reduce 阶段"></a>7.2 Map 和 Reduce 阶段</h2><p>将作业拆分成 Map 阶段和 Reduce 阶段</p>
<ol>
<li>准备 Map 处理的输入数据</li>
<li>Mapper 处理</li>
<li>Shuffle: 将相同的 key 分配到同一个 Reduce 节点</li>
<li>Reduce 处理</li>
<li>输出结果</li>
</ol>
<p><img src="https://user-images.githubusercontent.com/17758731/61185171-8ba32a00-a688-11e9-895f-b9b2e2fbaf96.png" alt="image"></p>
<p>假设现在有两个节点</p>
<ol>
<li>使用 <code>InputFormat</code> 读取文件系统(本地, HDFS), 并拆分成多个 Split</li>
<li>每个 Split 由一个 <code>RecordReader</code> 负责读取, 每读一行交由一个 mapper 处理</li>
<li>map 产生的结果交由 <code>Partitioner</code>, 将所有的 key 按一定规则分配给同一个节点并完成排序</li>
<li>相同的 key 交给 <code>Reduce</code> 负责处理</li>
<li>处理的结果交给 <code>OutputFormat</code> 写回文件系统</li>
</ol>
<h2 id="7-3-MapReduce-编程模型"><a href="#7-3-MapReduce-编程模型" class="headerlink" title="7.3 MapReduce 编程模型"></a>7.3 MapReduce 编程模型</h2><p><img src="https://user-images.githubusercontent.com/17758731/61185527-1b4ad780-a68d-11e9-9595-f8a380a57aa7.png" alt="image"></p>
<h3 id="7-3-1-Split"><a href="#7-3-1-Split" class="headerlink" title="7.3.1 Split"></a>7.3.1 Split</h3><p>被 InputFormat 从文件系统中读取并分片, 并交由 MapReduce 作业来处理的数据块</p>
<blockquote>
<p>HDFS 中的 blocksize 是 HDFS 中最小的存储单元, 默认 128M<br>Split 是 MapReduce 中最小的计算单元<br>默认情况下二者是一一对应的, 也可以手工设置二者之间的关系.</p>
</blockquote>
<h3 id="7-3-2-Combiner"><a href="#7-3-2-Combiner" class="headerlink" title="7.3.2 Combiner"></a>7.3.2 Combiner</h3><h3 id="7-3-3-Partitioner"><a href="#7-3-3-Partitioner" class="headerlink" title="7.3.3 Partitioner"></a>7.3.3 Partitioner</h3><h3 id="7-4-MapReduce-1-x-架构"><a href="#7-4-MapReduce-1-x-架构" class="headerlink" title="7.4 MapReduce 1.x 架构"></a>7.4 MapReduce 1.x 架构</h3><p><img src="https://user-images.githubusercontent.com/17758731/58748384-85773680-84aa-11e9-8233-daebe5f2ccf6.png" alt="image"></p>
<ol>
<li>JobTracker(JT)<ol>
<li>作业的管理者</li>
<li>将作业分解成多个任务(MapTask &amp; ReduceTask)</li>
<li>将任务分派给 TaskTracker 运行</li>
<li>作业监控, 容错处理</li>
<li>在一定时间内 JobTacker 没有收到 TaskTracker 的心跳, 会重新指派到其他的 TaskTracker 去执行</li>
</ol>
</li>
<li>TaskTracker(TT)<ol>
<li>任务的执行者, 执行任务(MapTask &amp; ReduceTask)</li>
<li>与 JobTracker 交互: 执行/启动/停止, 发送心跳信息给 JobTracker</li>
</ol>
</li>
<li>MapTask<ol>
<li>开发的 map 任务交由 MapTask 完成</li>
<li>解析每条记录的数据交给自己的 Map 方法处理</li>
<li>将 Map 的输出结果写到本地磁盘</li>
</ol>
</li>
<li>ReduceTask<ol>
<li>将 MapTask 输出的数据进行读取</li>
<li>按照数据进行分组传给 Reduce 方法处理</li>
<li>输出结果, 写入到 HDFS</li>
</ol>
</li>
</ol>
<h3 id="7-5-MapReduce-2-x-架构"><a href="#7-5-MapReduce-2-x-架构" class="headerlink" title="7.5 MapReduce 2.x 架构"></a>7.5 MapReduce 2.x 架构</h3><h3 id="7-6-Combiner"><a href="#7-6-Combiner" class="headerlink" title="7.6 Combiner"></a>7.6 Combiner</h3><ul>
<li>在 Mapper Task本地的 Reduce</li>
<li>减少 Map Task 输出的数据量及数据网络传输量</li>
<li>大部分情况下逻辑和 Reduce 基本相同</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/17758731/61586498-5fb40700-aba8-11e9-86ff-1309911e1010.png" alt="image"></p>
<h3 id="7-7-Partitioner"><a href="#7-7-Partitioner" class="headerlink" title="7.7 Partitioner"></a>7.7 Partitioner</h3><ul>
<li>决定 MapTask 输出的数据交由哪个 Reducer 处理</li>
<li>默认实现: 分发的 key 的 hash 值对 Reduce Task 个数取模</li>
</ul>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/blog/tags/Hadoop/" rel="tag"># Hadoop</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/blog/2019/04/13/Go自学笔记/" rel="next" title="Go自学笔记">
                <i class="fa fa-chevron-left"></i> Go自学笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/blog/2019/06/19/如何高效使用Vim/" rel="prev" title="如何高效使用Vim">
                如何高效使用Vim <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/blog/images/blog-logo.jpeg" alt="destiny">
            
              <p class="site-author-name" itemprop="name">destiny</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/blog/archives">
                
                    <span class="site-state-item-count">38</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/blog/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">21</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/blog/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">20</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/blog/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-简介"><span class="nav-text">1. 简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Hadoop-安装"><span class="nav-text">2. Hadoop 安装</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-JDK-安装配置"><span class="nav-text">2.1 JDK 安装配置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-Hadoop-安装配置"><span class="nav-text">2.2 Hadoop 安装配置</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Hadoop-配置"><span class="nav-text">3. Hadoop 配置</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-独立模式"><span class="nav-text">3.1 独立模式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-伪分布式"><span class="nav-text">3.2 伪分布式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-配置文件"><span class="nav-text">3.2.1 配置文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-配置-SSH"><span class="nav-text">3.2.2 配置 SSH</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-3-使用-HDFS"><span class="nav-text">3.2.3 使用 HDFS</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-1-启动"><span class="nav-text">3.2.3.1 启动</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-2-启动-yarn"><span class="nav-text">3.2.3.2 启动 yarn</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-完全分布式"><span class="nav-text">3.3 完全分布式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-1-准备工作"><span class="nav-text">3.3.1 准备工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-2-集群模式配置"><span class="nav-text">3.3.2 集群模式配置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-1-core-site-xml"><span class="nav-text">3.3.2.1 core-site.xml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-2-hdfs-site-xml"><span class="nav-text">3.3.2.2 hdfs-site.xml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-4-yarn-site-xml"><span class="nav-text">3.3.2.4 yarn-site.xml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-5-slaves"><span class="nav-text">3.3.2.5 slaves</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-分布式文件系统HDFS"><span class="nav-text">4. 分布式文件系统HDFS</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-概念"><span class="nav-text">4.1 概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-设计目标"><span class="nav-text">4.2 设计目标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-HDFS-架构"><span class="nav-text">4.3 HDFS 架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-HDFS-副本机制"><span class="nav-text">4.4 HDFS 副本机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-5-Hadoop-Shell"><span class="nav-text">4.5 Hadoop Shell</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-6-通过代码操作-API"><span class="nav-text">4.6 通过代码操作 API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-1-测试文件的通用代码"><span class="nav-text">4.6.1 测试文件的通用代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-2-创建路径"><span class="nav-text">4.6.2 创建路径</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-3-创建文件"><span class="nav-text">4.6.3 创建文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-4-查看文件内容"><span class="nav-text">4.6.4 查看文件内容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-5-重命名"><span class="nav-text">4.6.5 重命名</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-6-将本地文件-copy-到-HDFS"><span class="nav-text">4.6.6 将本地文件 copy 到 HDFS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-7-带进度条的上传"><span class="nav-text">4.6.7 带进度条的上传</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-8-下载-HDFS-文件"><span class="nav-text">4.6.8 下载 HDFS 文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-9-查看指定路径下的所有文件"><span class="nav-text">4.6.9 查看指定路径下的所有文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-10-删除文件"><span class="nav-text">4.6.10 删除文件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-7-HDFS-文件写入流程"><span class="nav-text">4.7 HDFS 文件写入流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-8-HDFS-文件读取刘晨"><span class="nav-text">4.8 HDFS 文件读取刘晨</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-资源调度框架-YARN"><span class="nav-text">5. 资源调度框架 YARN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-背景"><span class="nav-text">5.1 背景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-1-MapReduce-1-X-存在的问题"><span class="nav-text">5.1.1 MapReduce 1.X 存在的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-2-资源利用-amp-运维成本"><span class="nav-text">5.1.2 资源利用&amp;运维成本</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-架构"><span class="nav-text">5.2 架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-执行流程"><span class="nav-text">5.3 执行流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-提交作业到-YARN-上执行"><span class="nav-text">5.5 提交作业到 YARN 上执行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-1-提交-Hadoop-包的-Example"><span class="nav-text">5.5.1 提交 Hadoop 包的 Example</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-启动脚本分析"><span class="nav-text">6. 启动脚本分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-start-all-sh-启动分析"><span class="nav-text">6.1 start-all.sh 启动分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-hadoop-config-sh"><span class="nav-text">6.2 hadoop-config.sh</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-MapReduce"><span class="nav-text">7. MapReduce</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#7-2-Map-和-Reduce-阶段"><span class="nav-text">7.2 Map 和 Reduce 阶段</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-3-MapReduce-编程模型"><span class="nav-text">7.3 MapReduce 编程模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-1-Split"><span class="nav-text">7.3.1 Split</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-2-Combiner"><span class="nav-text">7.3.2 Combiner</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-3-Partitioner"><span class="nav-text">7.3.3 Partitioner</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-MapReduce-1-x-架构"><span class="nav-text">7.4 MapReduce 1.x 架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-MapReduce-2-x-架构"><span class="nav-text">7.5 MapReduce 2.x 架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-6-Combiner"><span class="nav-text">7.6 Combiner</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-7-Partitioner"><span class="nav-text">7.7 Partitioner</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate"> 
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">destiny</span>

  

  
</div>




  <div class="powered-by">
  <i class="fa fa-user-md"></i>
  <span id="busuanzi_container_site_uv">本站访客数:<span id="busuanzi_value_site_uv"></span></span>
  
  <!-- 由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动 -->
  </div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Code Alone </div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/blog/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=6.1.0"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=6.1.0"></script>



  
  


  <script type="text/javascript" src="/blog/js/src/affix.js?v=6.1.0"></script>

  <script type="text/javascript" src="/blog/js/src/schemes/pisces.js?v=6.1.0"></script>



  
  <script type="text/javascript" src="/blog/js/src/scrollspy.js?v=6.1.0"></script>
<script type="text/javascript" src="/blog/js/src/post-details.js?v=6.1.0"></script>



  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=6.1.0"></script>



  



	





  





  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/blog/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  
  

  

  

  

  

</body>
</html>

<script type="text/javascript" src="/js/src/love.js"></script>>